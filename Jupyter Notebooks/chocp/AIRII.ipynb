{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d9fa29-87ab-406c-87f4-b396bfb1dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.subplots as sub\n",
    "import dash\n",
    "from dash import dcc, html, Output, Input, State\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support, confusion_matrix, top_k_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "#from optuna.integration import PyTorchIgnitePruningHandler\n",
    "\n",
    "from functools import partial\n",
    "import random\n",
    "import os\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from torchvision.models import vgg16_bn, resnet50, resnet18, efficientnet_b0, densenet121, ResNet50_Weights, ResNet18_Weights, VGG16_BN_Weights, DenseNet121_Weights, EfficientNet_B0_Weights\n",
    "from torchvision.utils import make_grid, draw_bounding_boxes, draw_segmentation_masks, draw_keypoints\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, v2, ToPILImage\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "# from ignite.metrics import Accuracy, Loss, RunningAverage, ConfusionMatrix\n",
    "# from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, AutoModelForImageClassification, AutoImageProcessor, Trainer, TrainingArguments\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "import socket\n",
    "import json\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd35491-c1e0-493f-b5f6-f9354704a2c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bf6792ece449e29201d5e6a4ccfa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starting from chocp/, go up two levels to parent_dir/\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "# Prepend to sys.path so Python can find src/\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "\n",
    "\n",
    "from src.chocp_dataset import FGVCAircraftDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e4b261-5886-4d8c-8d68-1de95e0c463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edab239-5715-44e2-9d4c-065ca5d186c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN Backbone\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=30, attention_module=None):#, p_conv = 0.2, p_fc = 0.5\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # 224x224 -> 224x224\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 112x112\n",
    "            #nn.Dropout2d(p_conv), # spatial dropout\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # 112x112\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 56x56\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # 56x56\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 28x28\n",
    "            #nn.Dropout2d(p_conv),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # 28x28\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),  # 14x14\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # B, 1024, 1, 1\n",
    "        )\n",
    "        self.attention = attention_module(1024) if attention_module else nn.Identity() #if attention_module else nn.Identity()\n",
    "        #self.dropout_fc = nn.Dropout(p_fc)\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) #Bx1024x1x1\n",
    "        #if self.attention:\n",
    "        x = self.attention(x) # or Identity\n",
    "        x = x.view(x.size(0), -1) #Bx1024\n",
    "        #x = self.dropout_fc(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baff9845-542d-4493-8c86-5f39cba8ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BCNN Module\n",
    "class BCNN(nn.Module):\n",
    "    def __init__(self, num_classes=30, attention_module=None):\n",
    "        super(BCNN, self).__init__()\n",
    "\n",
    "        # Custom feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # [B, 64, 224, 224]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B, 64, 112, 112]\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# [B, 128, 112, 112]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B, 128, 56, 56]\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# [B, 256, 56, 56]\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                 # [B, 256, 1, 1]\n",
    "        )\n",
    "        \n",
    "        self.attention = attention_module(256) if attention_module else nn.Identity()\n",
    "        self.output_dim = 256\n",
    "        self.fc = nn.Linear(self.output_dim * self.output_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # [B, 256, 1, 1]\n",
    "        x = self.attention(x) # Apply attention if provided\n",
    "        x = x.view(x.size(0), self.output_dim)  # [B, 256]\n",
    "\n",
    "        # Bilinear pooling\n",
    "        x = torch.bmm(x.unsqueeze(2), x.unsqueeze(1))  # [B, 256, 256]\n",
    "        x = x.view(x.size(0), -1)  # [B, 256*256]\n",
    "\n",
    "        # Signed square root normalization\n",
    "        x = torch.sign(x) * torch.sqrt(torch.abs(x) + 1e-10)\n",
    "\n",
    "        # L2 normalization\n",
    "        x = F.normalize(x)\n",
    "\n",
    "        # Classification\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23ea53c-94da-442a-851e-e74fea61de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAP Module\n",
    "class CAP(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(CAP, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = self.global_avg_pool(x)\n",
    "        attention = self.fc1(context)\n",
    "        attention = self.relu(attention)\n",
    "        attention = self.fc2(attention)\n",
    "        attention = self.sigmoid(attention)\n",
    "        return x * attention\n",
    "\n",
    "# SE Module\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(b, c)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# CBAM Module\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        # Channel Attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel Attention\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        channel_att = self.sigmoid_channel(avg_out + max_out)\n",
    "        x = x * channel_att\n",
    "\n",
    "        # Spatial Attention\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_att = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        spatial_att = self.sigmoid_spatial(self.conv_spatial(spatial_att))\n",
    "        x = x * spatial_att\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6890f5a2-daa8-4ec4-96c3-0c514dedbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPResNet(nn.Module):\n",
    "    def __init__(self, num_classes=30, drop=0.0):\n",
    "        super(CAPResNet, self).__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.cap = CAP(in_channels=2048)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.cap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ResNet50 with SE Block\n",
    "class SEEffNet(nn.Module):\n",
    "    def __init__(self, num_classes=30, drop=0.0):\n",
    "        super(SEEffNet, self).__init__()\n",
    "        self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.backbone.classifier = nn.Identity() # fc if resnet\n",
    "        self.se = SEBlock(in_channels=1280) #2048 for resnet, 1280 for efficientnet, densenet121 = 1024, final layer also classifier\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.classifier = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.se(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ResNet50 with CBAM Block\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.cbam = CBAM(in_channels=2048)\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.cbam(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ResNet50 with BCNN Block (Bilinear CNN)\n",
    "class BCNNResNet(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(BCNNResNet, self).__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity() # Remove final fully connected layer\n",
    "        self.output_dim = 2048 # resnet50 final feature depth\n",
    "        self.classifier = nn.Linear(self.output_dim * self.output_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x) # extract features Bx2048\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1) # Bx2048x1x1\n",
    "        x = x.view(x.size(0), self.output_dim) # Bx2048\n",
    "        x = torch.bmm(x.unsqueeze(2), x.unsqueeze(1)) # Bilinear pooling\n",
    "        x = x.view(x.size(0), -1) # Bx2048*2048\n",
    "        x = torch.sign(x) * torch.sqrt(torch.abs(x) + 1e-10) # signed square root normalization\n",
    "        x = F.normalize(x) # L2 normalization\n",
    "        x = self.classifier(x) # classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05685857-cf10-4013-8297-838bc6cf44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_probs = nn.functional.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = nn.functional.cross_entropy(pred, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "919c1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(img_size = 224, batch_size = 32, annot = 'manufacturer'):\n",
    "    mean=[0.485,0.456,0.406]\n",
    "    std=[0.229,0.224,0.225]\n",
    "\n",
    "    train_tf = A.Compose([\n",
    "        A.RandomResizedCrop((img_size, img_size)),  # A.Resize(img_size, img_size)\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-15, 15), p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    test_tf = A.Compose([\n",
    "        #A.LongestMaxSize(max_size=256)\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = FGVCAircraftDataset(root=\"C:/Users/chihp/UMich/SIADS/699/FGVC/fgvc-aircraft-2013b/data\", split='train', level = annot, transform=train_tf, \n",
    "                                        return_class=False, cropped=True, album = True)\n",
    "    val_dataset   = FGVCAircraftDataset(root=\"C:/Users/chihp/UMich/SIADS/699/FGVC/fgvc-aircraft-2013b/data\", split='val', level = annot, transform=test_tf, \n",
    "                                        return_class=False, cropped=False, album = True)\n",
    "    test_dataset   = FGVCAircraftDataset(root=\"C:/Users/chihp/UMich/SIADS/699/FGVC/fgvc-aircraft-2013b/data\", split='test', level = annot, transform=test_tf, \n",
    "                                         return_class=False, cropped=False, album = True)\n",
    "    class_names = train_dataset.classes\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader, num_classes, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58aa0bd-85ed-4028-8bc3-8c161fabd4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(img_size = 224, batch_size = 32, annot = 'manufacturer'):\n",
    "    mean=[0.485,0.456,0.406]\n",
    "    std=[0.229,0.224,0.225]\n",
    "\n",
    "    train_tf = A.Compose([\n",
    "        A.RandomResizedCrop((img_size, img_size)),  # A.Resize(img_size, img_size)\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-15, 15), p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    test_tf = A.Compose([\n",
    "        #A.LongestMaxSize(max_size=256)\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = FGVCAircraftDataset(root=\"C:/Users/chihp/UMich/SIADS/699/FGVC/fgvc-aircraft-2013b/data\", split='train', level = annot, transform=train_tf, \n",
    "                                        return_class=False, cropped=True, album = True)\n",
    "    val_dataset   = FGVCAircraftDataset(root=\"C:/Users/chihp/UMich/SIADS/699/FGVC/fgvc-aircraft-2013b/data\", split='val', level = annot, transform=test_tf, \n",
    "                                        return_class=False, cropped=False, album = True)\n",
    "    test_dataset   = FGVCAircraftDataset(root=\"C:/Users/chihp/UMich/SIADS/699/FGVC/fgvc-aircraft-2013b/data\", split='test', level = annot, transform=test_tf, \n",
    "                                         return_class=False, cropped=False, album = True)\n",
    "    class_names = train_dataset.classes\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e50c33-940b-4b10-b8aa-c58a139553f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation of final test set\n",
    "\n",
    "def compute_metrics(y_true, y_pred_logits, k=5):\n",
    "    \"\"\"\n",
    "    Computes Top-1 and Top-k accuracy, and macro/micro F1 scores.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (list or numpy array)\n",
    "    - y_pred_logits: Model output logits (tensor or numpy array)\n",
    "    - k: Value for Top-k accuracy\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Convert logits to predicted labels\n",
    "    y_pred_top1 = torch.argmax(y_pred_logits, dim=1).cpu().numpy()\n",
    "    y_true_np = y_true.cpu().numpy() if isinstance(y_true, torch.Tensor) else y_true\n",
    "    y_pred_np = y_pred_logits.cpu().numpy() if isinstance(y_pred_logits, torch.Tensor) else y_pred_logits\n",
    "\n",
    "    # Compute metrics\n",
    "    top1_acc = top_k_accuracy_score(y_true_np, y_pred_np, k=1)\n",
    "    topk_acc = top_k_accuracy_score(y_true_np, y_pred_np, k=k)\n",
    "    f1_macro = f1_score(y_true_np, y_pred_top1, average='macro')\n",
    "    f1_micro = f1_score(y_true_np, y_pred_top1, average='micro')\n",
    "\n",
    "    return {\n",
    "        'Top-1 Accuracy': top1_acc,\n",
    "        f'Top-{k} Accuracy': topk_acc,\n",
    "        'F1 Macro': f1_macro,\n",
    "        'F1 Micro': f1_micro\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d75ffa-b8d7-4629-97e8-3ef20d7ab2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_lst = []\n",
    "    acc_lst = []\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        #inputs, labels = cutmix_or_mixup(inputs, labels)#cutmix/mixup\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #hard_labels = labels.argmax(dim=1) #convert soft label to hard labels for cutmix or mixup\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)#\n",
    "        loss = criterion(outputs, labels)#\n",
    "        loss.backward()#\n",
    "        optimizer.step()#\n",
    "        # with autocast(device_type = 'cuda'):\n",
    "        #     outputs = model(inputs)\n",
    "        #     loss = criterion(outputs, labels)\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        \n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        #hard_labels = labels.argmax(dim=1) #convert soft label to hard labels for cutmix or mixup\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_lst.append(loss.item() * inputs.size(0))#\n",
    "        acc_lst.append(predicted.eq(labels).sum().item() / labels.size(0))\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    #loss_lst.append(epoch_loss)\n",
    "    #acc_lst.append(epoch_acc)\n",
    "    return epoch_loss, epoch_acc, loss_lst, acc_lst\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    loss_lst = []\n",
    "    acc_lst = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc = \"Evaluation\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            #hard_labels = labels.argmax(dim=1) #convert soft label to hard labels for cutmix or mixup\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            loss_lst.append(loss.item() * inputs.size(0))#\n",
    "            acc_lst.append(predicted.eq(labels).sum().item() / labels.size(0))\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    #loss_lst.append(epoch_loss)\n",
    "    #acc_lst.append(epoch_acc)\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels, loss_lst, acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7093da56-e3a0-4622-b6d3-dc9049536b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def unnormalize(img_tensor, mean, std):\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img_tensor\n",
    "\n",
    "def visualize_predictions(model, test_dataset, num_samples=10, normalized=True):\n",
    "    model.eval()\n",
    "    samples = random.sample(range(len(test_dataset)), num_samples)\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    inv_label_map = test_dataset.idx_to_class\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, sample_idx in enumerate(samples):\n",
    "            image, label = test_dataset[sample_idx]\n",
    "            input_img = image.unsqueeze(0).to(device)\n",
    "            output = model(input_img)\n",
    "            _, pred = torch.max(output, 1)\n",
    "\n",
    "            # Unnormalize if needed\n",
    "            if normalized:\n",
    "                image = unnormalize(image.clone(), mean, std)\n",
    "\n",
    "            img_disp = image.permute(1, 2, 0).cpu().numpy().clip(0, 1)\n",
    "            axes[idx].imshow(img_disp)\n",
    "            axes[idx].set_title(f\"Pred: {inv_label_map[pred.item()]}\\nActual: {inv_label_map[label]}\")\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68620d93-85e4-43bc-bc08-a776970a549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selector\n",
    "def get_model(backbone_name, num_classes, dropout_rate):\n",
    "    if backbone_name == \"ResNet50_CAP\":\n",
    "        return CAPResNet(num_classes=num_classes, drop=dropout_rate)\n",
    "    elif backbone_name == \"EffNet_SE\":\n",
    "        return SEEffNet(num_classes=num_classes, drop=dropout_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "\n",
    "# Objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    backbone_name = trial.suggest_categorical('backbone', ['ResNet50_CAP', 'EffNet_SE'])\n",
    "    scheduler_name = trial.suggest_categorical('scheduler', ['StepLR', 'CosineAnnealingLR', 'ReduceLROnPlateau'])\n",
    "    criterion_name = trial.suggest_categorical('criterion', ['CrossEntropy', 'LabelSmoothing', 'Focal'])\n",
    "\n",
    "    # Data\n",
    "    train_loader, val_loader, test_loader, num_classes, class_names = get_loaders(img_size=224, batch_size=batch_size, annot='variant')\n",
    "\n",
    "    # Model\n",
    "    model = get_model(backbone_name, num_classes, dropout_rate)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_cls = getattr(optim, optimizer_name)\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler\n",
    "    if scheduler_name == 'StepLR':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    elif scheduler_name == 'CosineAnnealingLR':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    elif scheduler_name == 'ReduceLROnPlateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
    "\n",
    "    # Criterion with hyperparameters\n",
    "    if criterion_name == 'CrossEntropy':\n",
    "        label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.2)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    elif criterion_name == 'LabelSmoothing':\n",
    "        smoothing = trial.suggest_float('smoothing', 0.05, 0.2)\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=smoothing)\n",
    "    elif criterion_name == 'Focal':\n",
    "        gamma = trial.suggest_float('gamma', 1.0, 3.0)\n",
    "        criterion = FocalLoss(gamma=gamma)\n",
    "\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    epochs_without_improvement = 0\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc, _, _ = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "        val_loss, val_acc, _, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if scheduler_name == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            print(\"✅ Model improved. Saving...\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"⚠️ No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925187c9-7104-4fe8-9151-48054c392a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(study_name=\"Finest_FGVCAircraft_Variant\", direction='maximize', \n",
    "#                             pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=3))\n",
    "# study.optimize(objective, timeout=None, n_jobs=1, n_trials=30, gc_after_trial=False, show_progress_bar=True)\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "# print(f\"  Accuracy: {trial.value}\")\n",
    "# print(\"  Params:\")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b397c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize optimization results\n",
    "# optuna.visualization.plot_optimization_history(study, target_name='Accuracy', error_bar=True).show()\n",
    "# optuna.visualization.plot_param_importances(study).show() # params=[\"x\", \"y\"]\n",
    "# optuna.visualization.plot_parallel_coordinate(study, target_name='Accuracy').show()\n",
    "# optuna.visualization.plot_slice(study, target_name='Accuracy').show()\n",
    "# optuna.visualization.plot_contour(study, target_name = 'Accuracy').show()\n",
    "# optuna.visualization.plot_edf(study, target_name = 'Accuracy').show()\n",
    "# optuna.visualization.plot_intermediate_values(study).show()\n",
    "# optuna.visualization.plot_rank(study, target_name = 'Accuracy').show()\n",
    "# optuna.visualization.timeline(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef07cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot with custom labels\n",
    "# plt.plot(range(len([t.value for t in study.trials if t.value is not None])), [t.value for t in study.trials if t.value is not None], marker='o')\n",
    "# plt.xlabel(\"Trial\")\n",
    "# plt.ylabel(\"Validation Accuracy\")\n",
    "# plt.title(\"Optimization History\")\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c194ab8-9c20-4f45-b2c9-2ef55d6f191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, num_classes, class_names = get_loaders(224,16,'variant')\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(224,16,'variant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62549eda-85f5-4a2d-bd4f-30797d8ee640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30ea979-c7be-48ef-94f5-6e2ceafe4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CAPResNet(num_classes, drop = 0.25).to(device)\n",
    "# Training loop with early stopping and adaptive learning rate\n",
    "patience = 8\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "num_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()#label_smoothing = 0.1)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=4.9122e-05, weight_decay=0.000126)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs)\n",
    "scaler = GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f1ce3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #criterion = SoftTargetCrossEntropy() # or nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.CrossEntropyLoss()#label_smoothing = 0.1)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "# #optimizer = optim.AdamW(model.parameters(), lr=0.000085, weight_decay = 0.000009)#70\n",
    "# #optimizer = optim.AdamW(model.parameters(), lr=0.000238, weight_decay = 0.00052)#30\n",
    "# #optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "# #optimizer = optim.SGD(cnn.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 0.001)\n",
    "# #optimizer = optim.SGD(filter(lambda p: p.requires_grad, res50.parameters()), lr = 0.001, momentum = 0.9, weight_decay = 0.001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7dbe1c-d635-44db-99aa-5e9edd076ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33381ddeb934e26803ad9eb079f011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32b5f2bc5a347ff9d79d33c8ca5ee93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 4.5360, Train Acc: 0.0294\n",
      "Val Loss: 4.3962, Val Acc: 0.0414\n",
      "✅ Model improved. Saving...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267a76e3edbb4d2487612735ee9ae13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "train_loss_lst =[]\n",
    "train_acc_lst = []\n",
    "val_loss_lst = []\n",
    "val_acc_lst = []\n",
    "all_val_preds = []\n",
    "all_val_labels = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc, t_loss_lst, t__acc_lst = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "    val_loss, val_acc, val_preds, val_labels, v_loss_lst, v_acc_lst = evaluate(model, val_loader, criterion, device)\n",
    "    train_loss_lst.append(train_loss)\n",
    "    train_acc_lst.append(train_acc)\n",
    "    val_loss_lst.append(val_loss)\n",
    "    val_acc_lst.append(val_acc)\n",
    "    \n",
    "    all_val_preds.extend(val_preds)\n",
    "    all_val_labels.extend(val_labels)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()#\n",
    "    #scheduler.step(val_loss) # only for 'ReduceLROnPlateau'\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n",
    "        print(\"✅ Model improved. Saving...\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"⚠️ No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"⏹️ Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32eb3f-be73-44a5-9e14-92cd9c201f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "final_preds = []\n",
    "final_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Final Evaluation\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        final_preds.append(outputs.cpu())\n",
    "        final_labels.append(labels.cpu())\n",
    "\n",
    "final_preds = torch.cat(final_preds)\n",
    "final_labels = torch.cat(final_labels)\n",
    "\n",
    "final_metrics = compute_metrics(final_labels, final_preds, k=5)\n",
    "print(\"Final Evaluation Metrics:\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483e8bf-51b7-480e-9c04-d6d9f7f90d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_val_labels, all_val_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "class_accuracy = np.diag(cm_normalized)\n",
    "tick_marks = np.arange(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38477506-3bdf-4891-ba98-995e7de5f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap using Plotly\n",
    "heatmap = go.Heatmap(\n",
    "    z=cm_normalized,\n",
    "    x=class_names,\n",
    "    y=class_names,\n",
    "    colorscale='Plasma',\n",
    "    colorbar=dict(title='Accuracy'),\n",
    "    hovertemplate='Actual: %{y}<br>Predicted: %{x}<br>Accuracy: %{z:.2%}<extra></extra>'\n",
    ")\n",
    "\n",
    "# Create scatter plot (lollipop style) for class accuracy\n",
    "scatter = go.Scatter(\n",
    "    x=class_names,\n",
    "    y=class_accuracy,\n",
    "    mode='markers+lines',\n",
    "    marker=dict(size=8, color='blue'),\n",
    "    line=dict(color='lightblue'),\n",
    "    name='Class Accuracy',\n",
    "    hovertemplate='Class: %{x}<br>Accuracy: %{y:.2%}<extra></extra>'\n",
    ")\n",
    "\n",
    "# Create subplots with corrected keyword 'shared_xaxes'\n",
    "fig = sub.make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    row_heights=[0.7, 0.3],\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    subplot_titles=(\"Normalized Confusion Matrix\", \"Accuracy per Class\")\n",
    ")\n",
    "\n",
    "fig.add_trace(heatmap, row=1, col=1)\n",
    "fig.add_trace(scatter, row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1200,\n",
    "    title_text=\"Interactive Confusion Matrix with Class Accuracy\",\n",
    "    xaxis2=dict(tickangle=90),\n",
    "    margin=dict(t=100)\n",
    ")\n",
    "\n",
    "fig.write_html(\"interactive_confusion_matrix.html\")\n",
    "print(\"Interactive confusion matrix with class accuracy saved as 'interactive_confusion_matrix.html'.\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553bddf-3a0e-4913-9c80-55b533146c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Single Batch Loss: 138.283966  [   32/ 3334]\n",
      "Single Batch Loss: 121.316643  [ 3232/ 3334]\n",
      "Epoch 1/10, Average Train Loss: 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 90.9136 \n",
      " Accuracy: 21.9% \n",
      " Accuracy: 0.2193, Precision: 0.0852, Recall: 0.2193, F1 Score: 0.0894\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Single Batch Loss: 106.701195  [   32/ 3334]\n",
      "Single Batch Loss: 114.133286  [ 3232/ 3334]\n",
      "Epoch 2/10, Average Train Loss: 0.0342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 90.7384 \n",
      " Accuracy: 20.3% \n",
      " Accuracy: 0.2034, Precision: 0.1091, Recall: 0.2034, F1 Score: 0.1014\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Single Batch Loss: 115.209602  [   32/ 3334]\n",
      "Single Batch Loss: 100.310944  [ 3232/ 3334]\n",
      "Epoch 3/10, Average Train Loss: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 92.8592 \n",
      " Accuracy: 19.6% \n",
      " Accuracy: 0.1962, Precision: 0.0713, Recall: 0.1962, F1 Score: 0.0963\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Single Batch Loss: 107.738258  [   32/ 3334]\n",
      "Single Batch Loss: 106.960472  [ 3232/ 3334]\n",
      "Epoch 4/10, Average Train Loss: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 90.2780 \n",
      " Accuracy: 21.7% \n",
      " Accuracy: 0.2169, Precision: 0.1272, Recall: 0.2169, F1 Score: 0.0916\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Single Batch Loss: 98.908798  [   32/ 3334]\n",
      "Single Batch Loss: 94.587761  [ 3232/ 3334]\n",
      "Epoch 5/10, Average Train Loss: 0.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 90.2738 \n",
      " Accuracy: 21.5% \n",
      " Accuracy: 0.2145, Precision: 0.0865, Recall: 0.2145, F1 Score: 0.1054\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Single Batch Loss: 114.875221  [   32/ 3334]\n",
      "Single Batch Loss: 104.508957  [ 3232/ 3334]\n",
      "Epoch 6/10, Average Train Loss: 0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 88.4118 \n",
      " Accuracy: 21.4% \n",
      " Accuracy: 0.2139, Precision: 0.0970, Recall: 0.2139, F1 Score: 0.1021\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Single Batch Loss: 101.030960  [   32/ 3334]\n",
      "Single Batch Loss: 96.518448  [ 3232/ 3334]\n",
      "Epoch 7/10, Average Train Loss: 0.0289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 85.7246 \n",
      " Accuracy: 20.1% \n",
      " Accuracy: 0.2010, Precision: 0.1452, Recall: 0.2010, F1 Score: 0.1297\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Single Batch Loss: 90.418419  [   32/ 3334]\n",
      "Single Batch Loss: 112.862831  [ 3232/ 3334]\n",
      "Epoch 8/10, Average Train Loss: 0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 85.6820 \n",
      " Accuracy: 21.7% \n",
      " Accuracy: 0.2169, Precision: 0.1493, Recall: 0.2169, F1 Score: 0.1281\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Single Batch Loss: 103.325729  [   32/ 3334]\n",
      "Single Batch Loss: 98.537682  [ 3232/ 3334]\n",
      "Epoch 9/10, Average Train Loss: 0.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 86.5654 \n",
      " Accuracy: 22.6% \n",
      " Accuracy: 0.2259, Precision: 0.1339, Recall: 0.2259, F1 Score: 0.1390\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Single Batch Loss: 101.197227  [   32/ 3334]\n",
      "Single Batch Loss: 92.590797  [ 3232/ 3334]\n",
      "Epoch 10/10, Average Train Loss: 0.0278\n",
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 85.1357 \n",
      " Accuracy: 23.2% \n",
      " Accuracy: 0.2319, Precision: 0.1374, Recall: 0.2319, F1 Score: 0.1158\n",
      "Done!\n",
      "CPU times: total: 48min 11s\n",
      "Wall time: 45min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Sample data (replace these lists with actual collected data)\n",
    "# train_losses = [2.0, 1.8, 1.5, 1.2, 1.0, 0.8, 0.6, 0.5, 0.4, 0.3]\n",
    "# val_losses = [2.1, 1.9, 1.6, 1.3, 1.1, 0.9, 0.7, 0.6, 0.5, 0.4]\n",
    "# train_accuracies = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "# val_accuracies = [0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "\n",
    "ep = range(1, len(train_loss_lst) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ep, train_loss_lst, label='Training Loss')\n",
    "plt.plot(ep, val_loss_lst, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ep, train_acc_lst, label='Training Accuracy')\n",
    "plt.plot(ep, val_acc_lst, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e01ee-a907-4ba0-a7a0-a13996028aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Average Validation/ Test Loss: \n",
      " 85.4091 \n",
      " Accuracy: 23.3% \n",
      " Accuracy: 0.2334, Precision: 0.1407, Recall: 0.2334, F1 Score: 0.1152\n",
      "CPU times: total: 1min 52s\n",
      "Wall time: 1min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85.40913444700695"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_predictions(model, test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
